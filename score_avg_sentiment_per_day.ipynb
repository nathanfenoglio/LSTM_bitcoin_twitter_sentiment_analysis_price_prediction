{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MzYo5JXWg94G"
   },
   "outputs": [],
   "source": [
    "# program to score average sentiment with weight added for # of likes and # of retweets using polyglot library for polarity scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xyPMusf0huxD"
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 124131,
     "status": "ok",
     "timestamp": 1682072061262,
     "user": {
      "displayName": "nathan fenoglio",
      "userId": "13879668938846553689"
     },
     "user_tz": 300
    },
    "id": "NbvNBSiWsvtY",
    "outputId": "f882b4ee-8e59-47b6-c749-abcdcbba08cb"
   },
   "outputs": [],
   "source": [
    "!pip install polyglot\n",
    "!pip install pyicu  \n",
    "!pip install Morfessor   \n",
    "!pip install pycld2      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2380,
     "status": "ok",
     "timestamp": 1682072063629,
     "user": {
      "displayName": "nathan fenoglio",
      "userId": "13879668938846553689"
     },
     "user_tz": 300
    },
    "id": "9Va20GY6tO3e",
    "outputId": "3c001912-29d6-48f4-926b-cd83ef3048cf"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "polyglot download ner2.en    \n",
    "polyglot download pos2.en    \n",
    "polyglot download sentiment2.en  \n",
    "polyglot download embeddings2.en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "15HX_Eryt2ow"
   },
   "outputs": [],
   "source": [
    "from polyglot.detect import Detector\n",
    "from polyglot.text import Text \n",
    "from polyglot.downloader import downloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 70803,
     "status": "ok",
     "timestamp": 1682072134418,
     "user": {
      "displayName": "nathan fenoglio",
      "userId": "13879668938846553689"
     },
     "user_tz": 300
    },
    "id": "mlgc0cuFuGIs",
    "outputId": "6e19729c-6ca6-4b27-82b9-cd04702b8f0c"
   },
   "outputs": [],
   "source": [
    "# downloading sentiment packages for a bunch of different languages\n",
    "!polyglot download sentiment2.af\n",
    "!polyglot download sentiment2.als\n",
    "!polyglot download sentiment2.am\n",
    "!polyglot download sentiment2.an\n",
    "!polyglot download sentiment2.ar\n",
    "!polyglot download sentiment2.arz\n",
    "!polyglot download sentiment2.as\n",
    "!polyglot download sentiment2.ast\n",
    "!polyglot download sentiment2.az\n",
    "!polyglot download sentiment2.ba\n",
    "!polyglot download sentiment2.bar\n",
    "!polyglot download sentiment2.be\n",
    "!polyglot download sentiment2.bg\n",
    "!polyglot download sentiment2.bn\n",
    "!polyglot download sentiment2.bo\n",
    "!polyglot download sentiment2.bpy\n",
    "!polyglot download sentiment2.br\n",
    "!polyglot download sentiment2.bs\n",
    "!polyglot download sentiment2.ca\n",
    "!polyglot download sentiment2.ce\n",
    "!polyglot download sentiment2.ceb\n",
    "!polyglot download sentiment2.cs\n",
    "!polyglot download sentiment2.cv\n",
    "!polyglot download sentiment2.cy\n",
    "!polyglot download sentiment2.da\n",
    "!polyglot download sentiment2.de\n",
    "!polyglot download sentiment2.diq\n",
    "!polyglot download sentiment2.dv\n",
    "!polyglot download sentiment2.el\n",
    "!polyglot download sentiment2.en\n",
    "!polyglot download sentiment2.eo\n",
    "!polyglot download sentiment2.es\n",
    "!polyglot download sentiment2.et\n",
    "!polyglot download sentiment2.eu\n",
    "!polyglot download sentiment2.fa\n",
    "!polyglot download sentiment2.fi\n",
    "!polyglot download sentiment2.fo\n",
    "!polyglot download sentiment2.fr\n",
    "!polyglot download sentiment2.fy\n",
    "!polyglot download sentiment2.ga\n",
    "!polyglot download sentiment2.gan\n",
    "!polyglot download sentiment2.gd\n",
    "!polyglot download sentiment2.gl\n",
    "!polyglot download sentiment2.gu\n",
    "!polyglot download sentiment2.gv\n",
    "!polyglot download sentiment2.he\n",
    "!polyglot download sentiment2.hi\n",
    "!polyglot download sentiment2.hif\n",
    "!polyglot download sentiment2.hr\n",
    "!polyglot download sentiment2.hsb\n",
    "!polyglot download sentiment2.ht\n",
    "!polyglot download sentiment2.hu\n",
    "!polyglot download sentiment2.hy\n",
    "!polyglot download sentiment2.ia\n",
    "!polyglot download sentiment2.id\n",
    "!polyglot download sentiment2.ilo\n",
    "!polyglot download sentiment2.io\n",
    "!polyglot download sentiment2.is\n",
    "!polyglot download sentiment2.it\n",
    "!polyglot download sentiment2.ja\n",
    "!polyglot download sentiment2.jv\n",
    "!polyglot download sentiment2.ka\n",
    "!polyglot download sentiment2.kk\n",
    "!polyglot download sentiment2.km\n",
    "!polyglot download sentiment2.kn\n",
    "!polyglot download sentiment2.ko\n",
    "!polyglot download sentiment2.ku\n",
    "!polyglot download sentiment2.ky\n",
    "!polyglot download sentiment2.la\n",
    "!polyglot download sentiment2.lb\n",
    "!polyglot download sentiment2.li\n",
    "!polyglot download sentiment2.lmo\n",
    "!polyglot download sentiment2.lt\n",
    "!polyglot download sentiment2.lv\n",
    "!polyglot download sentiment2.mg\n",
    "!polyglot download sentiment2.mk\n",
    "!polyglot download sentiment2.ml\n",
    "!polyglot download sentiment2.mn\n",
    "!polyglot download sentiment2.mr\n",
    "!polyglot download sentiment2.ms\n",
    "!polyglot download sentiment2.mt\n",
    "!polyglot download sentiment2.my\n",
    "!polyglot download sentiment2.ne\n",
    "!polyglot download sentiment2.nl\n",
    "!polyglot download sentiment2.nn\n",
    "!polyglot download sentiment2.no\n",
    "!polyglot download sentiment2.oc\n",
    "!polyglot download sentiment2.or\n",
    "!polyglot download sentiment2.os\n",
    "!polyglot download sentiment2.pa\n",
    "!polyglot download sentiment2.pam\n",
    "!polyglot download sentiment2.pl\n",
    "!polyglot download sentiment2.pms\n",
    "!polyglot download sentiment2.ps\n",
    "!polyglot download sentiment2.pt\n",
    "!polyglot download sentiment2.qu\n",
    "!polyglot download sentiment2.rm\n",
    "!polyglot download sentiment2.ro\n",
    "!polyglot download sentiment2.ru\n",
    "!polyglot download sentiment2.sa\n",
    "!polyglot download sentiment2.sah\n",
    "!polyglot download sentiment2.scn\n",
    "!polyglot download sentiment2.sco\n",
    "!polyglot download sentiment2.se\n",
    "!polyglot download sentiment2.sh\n",
    "!polyglot download sentiment2.si\n",
    "!polyglot download sentiment2.sk\n",
    "!polyglot download sentiment2.sl\n",
    "!polyglot download sentiment2.sq\n",
    "!polyglot download sentiment2.sr\n",
    "!polyglot download sentiment2.su\n",
    "!polyglot download sentiment2.sv\n",
    "!polyglot download sentiment2.sw\n",
    "!polyglot download sentiment2.szl\n",
    "!polyglot download sentiment2.ta\n",
    "!polyglot download sentiment2.te\n",
    "!polyglot download sentiment2.tg\n",
    "!polyglot download sentiment2.th\n",
    "!polyglot download sentiment2.tk\n",
    "!polyglot download sentiment2.tl\n",
    "!polyglot download sentiment2.tr\n",
    "!polyglot download sentiment2.tt\n",
    "!polyglot download sentiment2.ug\n",
    "!polyglot download sentiment2.uk\n",
    "!polyglot download sentiment2.ur\n",
    "!polyglot download sentiment2.uz\n",
    "!polyglot download sentiment2.vec\n",
    "!polyglot download sentiment2.vi\n",
    "!polyglot download sentiment2.vls\n",
    "!polyglot download sentiment2.vo\n",
    "!polyglot download sentiment2.wa\n",
    "!polyglot download sentiment2.war\n",
    "!polyglot download sentiment2.yi\n",
    "!polyglot download sentiment2.yo\n",
    "!polyglot download sentiment2.zh\n",
    "!polyglot download sentiment2.zhw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 383,
     "status": "ok",
     "timestamp": 1682074390204,
     "user": {
      "displayName": "nathan fenoglio",
      "userId": "13879668938846553689"
     },
     "user_tz": 300
    },
    "id": "ohrLSm2Yh2Wo",
    "outputId": "53b4f233-f789-4549-d136-2e732613a02f"
   },
   "outputs": [],
   "source": [
    "# creating array of dates of the filenames that you will read in\n",
    "dates_for_csv_filenames = []\n",
    "for day in range(9, 21): \n",
    "    one_date = datetime.date(2023, 3, day)\n",
    "    dates_for_csv_filenames.append(one_date)\n",
    "\n",
    "print(dates_for_csv_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cshFRPxDplhK"
   },
   "outputs": [],
   "source": [
    "def get_polarities(df):\n",
    "    polarities = []\n",
    "    for i in range(len(df)):\n",
    "        # check for WARNING:polyglot.detect.base:Detector is not able to detect the language reliably\n",
    "        # and assign sentiment of 999 to discard later because tweet will not be useful to analyze\n",
    "        with warnings.catch_warnings(record=True) as w:\n",
    "            one_tweet_content_text = Text(str(df.iloc[i].loc['content']))\n",
    "            # strip text because sometimes there are spaces that are really just empty tweets after certain things have been removed\n",
    "            # strip only strips the leading and trailing whitespace from strings\n",
    "            one_tweet_content_text = one_tweet_content_text.strip()\n",
    "            if w:\n",
    "                polarities.append('999')\n",
    "\n",
    "        # doesn't seem that any tweets get assigned sentiment == 333\n",
    "        # think because a tweet of length 0 would have already triggered the warning and would have been assigned sentiment = 999\n",
    "        # now it is since applying strip function to the text\n",
    "        if len(one_tweet_content_text) == 0: \n",
    "            print('len(one_tweet_content_text) == 0')\n",
    "            polarity = 333\n",
    "        try:\n",
    "            polarity = one_tweet_content_text.polarity\n",
    "            detector = Detector(str(df.iloc[i].loc['content']))\n",
    "        except ZeroDivisionError: \n",
    "            polarity = 0\n",
    "        except Exception as e:\n",
    "            polarity = 999\n",
    "\n",
    "        polarities.append(polarity)\n",
    "\n",
    "    # add column for original calculated polarities without factoring in # of likes or # of retweets \n",
    "    df['sentiment'] = polarities\n",
    "\n",
    "    # cleaning data\n",
    "\n",
    "    # dropping all rows with sentiment == 999 because mainly tweets with just numbers for content and other tweets not useful for analysis\n",
    "    print(len(df))\n",
    "    mask_999 = df['sentiment'] == 999\n",
    "    df = df[~mask_999]\n",
    "    print(len(df))\n",
    "\n",
    "    mask_333 = df['sentiment'] == 333\n",
    "    print(df[mask_333])\n",
    "    print(len(df[mask_333]))\n",
    "\n",
    "    num_content_nans = df['content'].isna().sum()\n",
    "    print(f\"num_content_nans: {num_content_nans}\") \n",
    "    # drop rows with NaN values in the 'content' column\n",
    "    df = df.dropna(subset=['content'])\n",
    "\n",
    "    # create a column of just the dates not the time \n",
    "    just_dates = [df.iloc[i].loc['datetime'].split()[0] for i in range(len(df))]\n",
    "    df['just_date'] = just_dates \n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "liMNoy_D2w_d"
   },
   "outputs": [],
   "source": [
    "def create_col_sentiment_likes_retweets_combo(df):\n",
    "    # experimenting with factoring in the # of likes and # of retweets of a tweet into its sentiment score\n",
    "    num_likes_normalized_plus_num_retweets_normalized = (df['num_likes_normalized']) + (df['num_retweets_normalized'])\n",
    "\n",
    "    sentiment_num_likes_num_retweets = []\n",
    "    # can mess with this # to weight how much the number of likes and number of retweets factors into the sentiment score, \n",
    "    weight_of_likes_retweets_in_sentiment_score = 10\n",
    "    for i in range(len(df)): # maybe better pandas or numpy way to do this instead of with for loop\n",
    "        if df.iloc[i].loc['sentiment'] > 0:\n",
    "            sentiment_num_likes_num_retweets.append(df.iloc[i].loc['sentiment'] + (weight_of_likes_retweets_in_sentiment_score * num_likes_normalized_plus_num_retweets_normalized.iloc[i]))\n",
    "        elif df.iloc[i].loc['sentiment'] < 0:\n",
    "            sentiment_num_likes_num_retweets.append(df.iloc[i].loc['sentiment'] - (weight_of_likes_retweets_in_sentiment_score * num_likes_normalized_plus_num_retweets_normalized.iloc[i]))\n",
    "        else:\n",
    "            sentiment_num_likes_num_retweets.append(0)\n",
    "\n",
    "    df['sentiment_num_likes_num_retweets'] = np.array(sentiment_num_likes_num_retweets)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LAt28dtKi9l7"
   },
   "outputs": [],
   "source": [
    "def get_day_from_file_and_preprocess(filename):\n",
    "    one_file_df = pd.read_csv(one_day_filename)\n",
    "    # drop 'Unnamed:' column is actually just unnamed and is the number index of the row in the csv file\n",
    "    one_file_df.drop(one_file_df.columns[0], axis = 1, inplace=True)\n",
    "    one_file_df = one_file_df.drop_duplicates(keep='first') # I don't believe there should be any duplicate tweets anymore, but just in case\n",
    "\n",
    "    # check for nans in content (the tweet itself) and drop rows if they exist\n",
    "    num_content_nans = one_file_df['content'].isna().sum()\n",
    "    print(num_content_nans) \n",
    "    # drop rows with NaN values in the 'content' column\n",
    "    one_file_df = one_file_df.dropna(subset=['content'])\n",
    "\n",
    "    # getting just the dates without the times from datetime to be able to group by date\n",
    "    just_dates = [one_file_df.iloc[i].loc['datetime'].split()[0] for i in range(len(one_file_df))]\n",
    "    one_file_df['just_date'] = just_dates\n",
    "\n",
    "    one_file_df = get_polarities(one_file_df)\n",
    "\n",
    "    # normalize num_likes and num_retweets and add columns to the dataframe for each normalized version\n",
    "    min_num_likes = one_file_df['num_likes'].min()\n",
    "    max_num_likes = one_file_df['num_likes'].max()\n",
    "    print(min_num_likes)\n",
    "    print(max_num_likes)\n",
    "\n",
    "    # normalizing the # of likes column\n",
    "    one_file_df['num_likes_normalized'] = (one_file_df['num_likes'] - min_num_likes) / (max_num_likes - min_num_likes)\n",
    "\n",
    "    min_num_retweets = one_file_df['num_retweets'].min()\n",
    "    max_num_retweets = one_file_df['num_retweets'].max()\n",
    "    print(min_num_retweets)\n",
    "    print(max_num_retweets)\n",
    "\n",
    "    # normalizing the # of retweets column\n",
    "    one_file_df['num_retweets_normalized'] = (one_file_df['num_retweets'] - min_num_retweets) / (max_num_retweets - min_num_retweets)  \n",
    "\n",
    "    # create column trying to combine original sentiment with # of likes and # of retweets in a useful way \n",
    "    one_file_df = create_col_sentiment_likes_retweets_combo(one_file_df)\n",
    "\n",
    "    return one_file_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5211367,
     "status": "ok",
     "timestamp": 1682079630303,
     "user": {
      "displayName": "nathan fenoglio",
      "userId": "13879668938846553689"
     },
     "user_tz": 300
    },
    "id": "SPmD6B7OhTJ6",
    "outputId": "e40c6210-9403-497b-b1f0-2ba04fec61fe"
   },
   "outputs": [],
   "source": [
    "# read files into list of separate dataframes per date \n",
    "dataframes_by_day = []\n",
    "for i in range(len(dates_for_csv_filenames)):\n",
    "    one_day_filename = \"bitcoin_just_\" + dates_for_csv_filenames[i].strftime(\"%m\"+\".\"+\"%d\"+\".\"+\"%Y\") + \".csv\"\n",
    "    print(one_day_filename)\n",
    "    # doing preprocessing as well here to avoid looping back through\n",
    "    one_file_df = get_day_from_file_and_preprocess(one_day_filename)\n",
    "    dataframes_by_day.append(one_file_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2oLiZfNw6qUG"
   },
   "outputs": [],
   "source": [
    "day_info_summaries = [] # array to hold all of the dictionaries\n",
    "# dictionaries will contain key:value pairs \n",
    "# {KEY-DATE: VALUE-DATE, KEY-MEAN_WEIGHTED_SENTIMENT: VALUE-AVERAGE_SENTIMENT, KEY-NUM_TWEETS: VALUE-NUM_TWEETS}\n",
    "for dataframe in dataframes_by_day:\n",
    "    one_day_info_dict = {}\n",
    "    one_day_info_dict['date'] = dataframe.iloc[0].loc['just_date']\n",
    "    one_day_info_dict['mean_weighted_sentiment'] = dataframe['sentiment_num_likes_num_retweets'].mean()\n",
    "    one_day_info_dict['num_tweets'] = len(dataframe)\n",
    "    day_info_summaries.append(one_day_info_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 26,
     "status": "ok",
     "timestamp": 1682079630305,
     "user": {
      "displayName": "nathan fenoglio",
      "userId": "13879668938846553689"
     },
     "user_tz": 300
    },
    "id": "EWUmiJphBFaQ",
    "outputId": "138ea226-f310-414c-f3ab-192079c11e77"
   },
   "outputs": [],
   "source": [
    "print(day_info_summaries) # seems to work for getting the info that you want per day in dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8KbMHsvXBgBg"
   },
   "outputs": [],
   "source": [
    "# seems to work out well, do for all of the days that you have saved\n",
    "filename = 'summary_per_day.csv'\n",
    "with open(filename, \"w\", newline=\"\") as csv_file:\n",
    "    writer = csv.DictWriter(csv_file, fieldnames=day_info_summaries[0].keys())\n",
    "\n",
    "    # write the header row one time \n",
    "    writer.writeheader()\n",
    "\n",
    "    # then each dictionary without the header on the following lines\n",
    "    for one_day_dict in day_info_summaries:\n",
    "        writer.writerow(one_day_dict)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNCj+t7qs5e6q107lZVMPGP",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
